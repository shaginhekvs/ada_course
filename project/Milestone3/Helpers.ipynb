{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "In this notebook, we define the set of helper functions that we use to help in the analysis. Given the length of the notebook, we have decided to separate these functions from the analysis itself to make it more readable.\n",
    "\n",
    "The rest of this notebook is organized as follows. We have divided it in sections, depending on what the functions fo. Thus, we have::\n",
    "* [Tweets processing and reading](#processing): for processing the tweets read from the database and the APIs and also defining the preprocessing functions used internally.\n",
    "* [Basic analysis](#basic): functions for plotting number of tweets and retweets\n",
    "* [Language](#language): analysis of the different languages of the tweets\n",
    "* [Topics modelling](#topics): function for obtaining the different topics discussed on the tweets so __we can observe how they tend to change over time and what is discussed__\n",
    "* [Hashtag analysis](#hashtag): different functions for creating wordclouds of hashtags and studying coocurrence of hashtags theoretically confronted\n",
    "* [Sentiment analysis](#sentiment): functions performing sentiment analysis of the tweets that allow us to study the polarity and subjectivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import re\n",
    "import pandas as pd\n",
    "import string \n",
    "import pickle\n",
    "import os\n",
    "import codecs\n",
    "import io\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "import datetime as dt\n",
    "import json\n",
    "import time\n",
    "import numpy\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "import random\n",
    "from functools import reduce\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "#Error message if an import fails\n",
    "some_failed=False\n",
    "\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "except:\n",
    "    !conda install -y -v -c conda-forge textblob\n",
    "    some_failed=True\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "except:\n",
    "    !conda install -y -v -c conda-forge nltk\n",
    "    some_failed=True\n",
    "try:\n",
    "    import gensim\n",
    "    from gensim import corpora\n",
    "    from gensim.models import LdaMulticore\n",
    "except:\n",
    "    !conda install -y -v -c anaconda gensim\n",
    "    some_failed=True\n",
    "try:\n",
    "    from googletrans import Translator\n",
    "except:\n",
    "    !pip install googletrans\n",
    "    some_failed=True\n",
    "try: \n",
    "    import tweepy\n",
    "    from tweepy import OAuthHandler\n",
    "except:\n",
    "    !conda install -y -v -c conda-forge tweepy \n",
    "    some_failed=True\n",
    "try:\n",
    "    from wordcloud import WordCloud,STOPWORDS, ImageColorGenerator\n",
    "except:\n",
    "    !conda install -y -v -c https://conda.anaconda.org/amueller wordcloud\n",
    "    some_failed=True\n",
    "    \n",
    "# Learn about API authentication here: https://plot.ly/python/getting-started\n",
    "# Find your api_key here: https://plot.ly/settings/api\n",
    "try:\n",
    "    import plotly.plotly as py\n",
    "    import plotly.graph_objs as go\n",
    "except:\n",
    "    !pip install plotly\n",
    "    some_failed=True\n",
    "    \n",
    "try:\n",
    "    from sqlalchemy import create_engine\n",
    "except:\n",
    "    !pip install sqlalchemy\n",
    "    some_failed=True\n",
    "    \n",
    "if(some_failed):\n",
    "    print('restarted kernel with dependencies installed, please run cell again')\n",
    "    os._exit(00)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Constants\n",
    "class style:\n",
    "   BOLD = '\\033[1m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='processing'></a>\n",
    "## Tweets processing and reading\n",
    "\n",
    "In this section we define a series of functions used for reading tweets from the dataset, the Tweepy API and their basic processing.\n",
    "\n",
    "#### Database\n",
    "\n",
    " _Format of the tweets_\n",
    "\n",
    "The raw tweets obtained from the database have the following format: \n",
    "\n",
    "| 2 characters | 18 characters   | weekday month day  HH:MM:SS time_zone year| user | *optional* | text\n",
    "|:----:|:----:|:----:|:----:|:----:|:----:\n",
    "|   language id  | tweet id | date | username | (RT) | text\n",
    "\n",
    "\n",
    "The first two characters indicate the language of the tweet. Next is the 64 bit (18 characters) ID of the tweet. In the next field we can find the date, showing first weekday, followed by month, day of the month, hour , the time zone and finally the year. We can then find the tag *RT* if is a retweet. Finally, we have the tweet itself.\n",
    "\n",
    "_Reading_\n",
    "\n",
    "We create a function *read_tweet_files* for reading the tweets filtered by hashtag from the database. This function takes the hashtag used for filtering (to identify the file) and returns a list with one tweet per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_tweet_files(hashtag):\n",
    "    '''\n",
    "    Read tweets from the txt file gotten from cluster\n",
    "    INPUT:\n",
    "        hashtag: hashtags for which we have data\n",
    "    OUTPUT:\n",
    "        tweets_list,hashtag:list of tweets in file, hashtag searched for \n",
    "    '''\n",
    "    \n",
    "    tweets_list=[]\n",
    "    #path\n",
    "    file_name='../tweets_datasets_cluster/tweets'\n",
    "    #name of file depending on the hashtag\n",
    "    if hashtag=='#blacklivesmatter':\n",
    "        file_name+='_blm'\n",
    "    elif hashtag=='#alllivesmatter':\n",
    "        file_name+='_against_blm'\n",
    "    else:\n",
    "        return [],hashtag\n",
    "    file_name+='.txt'\n",
    "    \n",
    "    #read file\n",
    "    with io.open(file_name,'r',encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if(':') in line:\n",
    "                line=line.replace('\\\\t','\\t')\n",
    "                tweet_text=line.split(':',maxsplit=1)[1][:-2]\n",
    "                tweets_list.append(tweet_text)\n",
    "    return tweets_list,hashtag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Processing_\n",
    "\n",
    "To process these tweets in raw format we create the function *process_tweets* which given the list of tweets obtain from the MapReduce jobs, returns a dataframe which the tweets divided by their fields, that is with the columns:\n",
    "   * lang: language of the tweet\n",
    "   * id: id of the tweet, converted to int\n",
    "   * date: with the format year/month/day\n",
    "   * epoch: timestamp aggregated by day\n",
    "   * user: who tweeted\n",
    "   * text: of the tweet\n",
    "   \n",
    "In addition, we also add a column with the hashtags found in the tweets, the hashtag used in the filtering, and if it's a retweet the handle of the user who first tweeted:\n",
    "   * hashes_inside: hashtags found in the tweets\n",
    "   * hashtag: hashtag as filter to obtain the tweet\n",
    "   * to: retweet handles inside the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_tweets(tweet_list,hashtag):\n",
    "    '''\n",
    "    Process tweets from the database dividing them by their fields\n",
    "    INPUT:\n",
    "        tweet_list: tweet as returned by the databasee\n",
    "    OUTPUT:\n",
    "        dataframe with one row per tweet and columns\n",
    "            -lang: language of the tweet\n",
    "            -id: id of the tweet\n",
    "            -date: with the format year/month/day\n",
    "            -epoch: timestamp of the tweet aggregated by day\n",
    "            -user: who tweeted\n",
    "            -text: of the tweet\n",
    "            -hashtag:which tweet definitely contains\n",
    "            -hashes_inside: hashtags in the text\n",
    "            -to:retweet handles inside tweet \n",
    "    '''\n",
    "    \n",
    "    #Dictionary for storing intermediate values\n",
    "    d={'lang':[], 'id':[],'date':[], 'epoch':[], 'user':[], 'text':[],'hashtag':[],'hashes_inside':[],'to':[]}\n",
    "    #Iterate through every tweet in the list\n",
    "    for i,tweet in enumerate(tweet_list):\n",
    "        #Initalize values\n",
    "        lang,id_,day,month,year,epoch,user,to,hashes_in,text,dt='en','-1','-1','-1','-1','-1','-1','-1','-1','-1','-1'\n",
    "        try:\n",
    "            #Split them by spaces\n",
    "            token=tweet.split()\n",
    "\n",
    "            #Text: join the tokenized words separating them with spaces\n",
    "            text=\" \".join(token[9:])\n",
    "            #Language\n",
    "            lang=token[0]\n",
    "            #Tweet id\n",
    "            id_=int(token[1])\n",
    "            #Date\n",
    "            day=token[4]\n",
    "            month=token[3]\n",
    "            year=token[7]\n",
    "            dt = datetime.strptime(year+' '+month+' '+day, '%Y %b %d')\n",
    "            epoch=time.mktime(dt.timetuple())\n",
    "            #user who tweeted\n",
    "            user=token[8]\n",
    "            #Language\n",
    "            lang=token[0]\n",
    "            \n",
    "            \n",
    "            #Hashtags and user mentions\n",
    "            hashes_in='' #hashtags in text\n",
    "            to='' #user mentions\n",
    "            \n",
    "            #for each word in the text\n",
    "            for token in token[9:]:\n",
    "                #If it's a hasthag we add it to the list converting it to lowercase\n",
    "                if token[0]=='#':\n",
    "                    hashes_in=hashes_in+' '+token.lower()\n",
    "                #If it's a retweet, get the handles of the user who tweeted it \n",
    "                elif token[0]=='@':\n",
    "                    to_add=token\n",
    "                    if(to_add[-1])==':':\n",
    "                        to_add=to_add[:-1]\n",
    "                    to=to+' '+to_add\n",
    "\n",
    "        #If missing field, ignore tweet\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        #Language\n",
    "        d['lang'].append(lang[1:])\n",
    "        #ID\n",
    "        d['id'].append(id_)\n",
    "        \n",
    "        #Date\n",
    "        d['epoch'].append(epoch)\n",
    "        d['date'].append(dt)\n",
    "        #Username\n",
    "        d['user'].append(user)\n",
    "        #Text\n",
    "        d['text'].append(text)\n",
    "        d['hashtag'].append(hashtag)\n",
    "        d['hashes_inside'].append(hashes_in)\n",
    "        d['to'].append(to)\n",
    "    #Create dataframe from the dictionary and set the id as the index\n",
    "    df=pd.DataFrame.from_dict(d).set_index('id')\n",
    "    df['date']=pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter API\n",
    "\n",
    "In addition to the database, we also use the Twitter API to obtain data, as the database does not  have recent data and is missing information such as the number of retweets. To scrape the data we use the GetOldTweets-python API ( https://github.com/Jefferson-Henrique/GetOldTweets-python) saving the data in a sqlite database. \n",
    "\n",
    "We read the data using the read_tweets.py file (which can be found here) which saves them to the database. In addition, we also define the function *read_database* which reads each of the tables (one per protest) found in the SQLite database into a dataframe. \n",
    "\n",
    "We process the result with *conform_dataframes*, which process the dataframe received by the previous function to the same format as the rest of dataframes used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_database(table,engine=create_engine('sqlite:///../data.db')):\n",
    "    '''\n",
    "        Read the downloaded tweets from twitter saved in the database. \n",
    "        Table Names - Hashtags\n",
    "        INPUT:\n",
    "            table: table in which we have data\n",
    "        OUTPUT:\n",
    "            df: dataframe of the data from the table\n",
    "        1. blm - #blacklivesmatter\n",
    "        2. maga - #maga, #makeamericagreatagain\n",
    "        3. antimaga - #NotMyPresident , #TheResistance , #Resist , #TakeAKnee\n",
    "        4. misogyny -  #MeToo, #WHYWEMARCH,  #YesAllWomen,  #EverydaySexism, #WhyIStayed, #NotOkay, #VOTEPROCHOICE \n",
    "        5. bring_back - #bringbackourgirls, #ChibokGirls, #bbog\n",
    "        6. antblm  - #BlueLivesMatter, #AllLivesMatter, #WhiteLivesMattter, #NYPDLivesMatter,  #PoliceLivesMatter\n",
    "    '''\n",
    "    df=pd.read_sql_query('SELECT * FROM {}'.format(table),engine,parse_dates=True,index_col='id')\n",
    "    df.drop('geo',axis=1,inplace=True)\n",
    "    return df\n",
    "\n",
    "def conform_dataframes(dest,hashtag,cols_mapping=None):\n",
    "    '''\n",
    "        Read the downloaded tweets from twitter. \n",
    "        Table Names - Hashtags\n",
    "        INPUT:\n",
    "            table: table in which we have data\n",
    "        OUTPUT:\n",
    "            df: dataframe of the data from the table\n",
    "        1. blm - #blacklivesmatter\n",
    "        2. maga - #maga, #makeamericagreatagain\n",
    "        3. antimaga - #NotMyPresident , #TheResistance , #Resist , #TakeAKnee\n",
    "        4. misogyny -  #MeToo, #WHYWEMARCH,  #YesAllWomen,  #EverydaySexism, #WhyIStayed, #NotOkay, #VOTEPROCHOICE \n",
    "        5. bring_back - #bringbackourgirls, #ChibokGirls, #bbog\n",
    "        6. antblm  - #BlueLivesMatter, #AllLivesMatter, #WhiteLivesMattter, #NYPDLivesMatter,  #PoliceLivesMatter\n",
    "    '''\n",
    "    if cols_mapping==None:\n",
    "        cols_mapping={'mentions':'to','username':'user','hashtags':'hashes_inside'}\n",
    "        dest['hashtag']=hashtag\n",
    "        dest['epoch']=pd.to_datetime(dest['date']).apply(lambda x:time.mktime(x.to_pydatetime().timetuple()))\n",
    "    dest.rename(index=str, columns=cols_mapping,inplace=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the *load_dataframes* function, for the #BlackLivesMatter protest. As in this case, we have data from both the Twitter dataset from the cluster and the twitter API (for the 2016 and 2017 data), due to the more recent tweets not being present in the dataset present in the cluster. This function merges the output from both inputs into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataframes(hashtag):\n",
    "    '''\n",
    "        Read the blm and alllm tweets from database and csv. \n",
    "        \n",
    "        INPUT:\n",
    "            hashtag - Hashtag to read : possibly #blm or #alllm\n",
    "        OUTPUT:\n",
    "            df: dataframe of the data from the table and the csv\n",
    "    '''\n",
    "    to_do=False\n",
    "    table=''\n",
    "    tweet_str=''\n",
    "    if(hashtag=='#blm'):\n",
    "        tweet_str='#blacklivesmatter'\n",
    "        table='blm'\n",
    "        to_do=True\n",
    "    elif (hashtag=='#alllm'):\n",
    "        tweet_str='#alllivesmatter'\n",
    "        table='antblm'\n",
    "        to_do=True\n",
    "    else:\n",
    "        print('nothing to do')\n",
    "        to_do=False\n",
    "    if(to_do):\n",
    "        tweets_csv=process_tweets(*read_tweet_files(tweet_str))\n",
    "        df=read_database(table)\n",
    "        conform_dataframes(df,tweet_str)\n",
    "        df=tweets_csv.append(df)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        df['date']=pd.to_datetime(df.date)\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweepy API\n",
    "\n",
    "To obtain the missing values (e.g., number of retweets) from the cluster dataset we use the Tweepy API, passing it the indices of the tweets. \n",
    "\n",
    "We first define a function for loading the API, were you have to add the credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enter the credentials to access the twitter api\n",
    "def load_api():\n",
    "\n",
    "    consumer_key = ''\n",
    "    consumer_secret = ''\n",
    "    access_token = ''\n",
    "    access_secret = ''\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    # load the twitter API via tweepy\n",
    "    return tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the search of these columns we define the function *scrape_missing_columns*, passing it a list of indices to read. Since the Twitter API has some limitations of queries for each user per 15 min interval (explained at length on  https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object), we split the task of gathering additional info about the tweets, indicating with start and end which are indices to be read in this batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_tweets(tweets, filename):\n",
    "    ''' Function that appends tweets to a file. '''\n",
    " \n",
    "    with open(filename, 'a') as f:\n",
    "        for tweet in tweets:\n",
    "            json.dump(tweet._json, f)\n",
    "            f.write('\\n')\n",
    "            \n",
    "def scrape_missing_columns(list_of_ind, start,end):\n",
    "    '''\n",
    "    Read from the Tweepy API the missing fields\n",
    "    INPUT:\n",
    "        list_of_ind: list of indices to search for\n",
    "        start: first index to read\n",
    "        end: last index to read\n",
    "    \n",
    "    '''\n",
    "    list_of_ind=pd.DataFrame(list_of_ind)\n",
    "    # we wait 15 minutes if the a tweepy.TweepError which should be due to the limit of queries exceeded per user. \n",
    "\n",
    "    # one batch is allowed to gave 100 twitters / query\n",
    "    # MODIFY ACCORDINGLY idx_by_100 for every app\n",
    "    idx_by_100 = list(range(start, end, 100))\n",
    "    # distribution per single user, use 3 apps\n",
    "    # for each app define idx_by_100 to be data_split[0]. For consistent file naming, use the intervals such that their \n",
    "    # beginning value is divisible by 100 and their ending value ends in 999 or is the end (see above intervals)\n",
    "    data_split = np.linspace(start,end, 4)\n",
    "    # For each batch of 1000 tweets, we write them to a json file. \n",
    "    # we wait 15 minutes if the a tweepy.TweepError which should be due to the limit of queries exceeded per user. \n",
    "    last_tweet = idx_by_100[-1]\n",
    "    for count, idx_value in enumerate(idx_by_100):\n",
    "        last_tweet_batch = min((idx_value+100), last_tweet)\n",
    "        print(idx_value)\n",
    "        range_batch = list_of_ind[idx_value:last_tweet_batch]['id'].tolist()\n",
    "        filename = 'tweets_app'\n",
    "\n",
    "        try:\n",
    "            tweets_batch = api.statuses_lookup(range_batch)\n",
    "            write_tweets(tweets_batch, filename)\n",
    "            print('tweets recovered: ', len(tweets_batch), ' filename ', str(int(idx_value/1000) ))\n",
    "            print('progress: ', idx_value, ' out of ', last_tweet)\n",
    "        except tweepy.TweepError as e:\n",
    "            print(e)\n",
    "            print('exception raised, waiting 15 minutes')\n",
    "            print('(until:', dt.datetime.now()+dt.timedelta(minutes=15), ')')\n",
    "            time.sleep(15*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reading back in the tweets, we design the following procedure. \n",
    "\n",
    "i) we start with a dataframe with a single column representing the index of the tweets whose hashtag we are looking at, and that we obtained from the cluster. That column is the index of the dataframe. \n",
    "\n",
    "ii) we search through the json files collected from the api and we sequentially complete the relevant fields we are interested in in the dataframe. We dont read more than a fixed number of tweets at once because they eat up a lot of memory and we are only interested in a few fields. So we discard the tweets every once in a while.\n",
    "\n",
    "As the location of tweets is too sporadic, we are only interested in the number of retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_tweepy_result(df, hashtag):\n",
    "    '''\n",
    "    Reads the json files obtain with Tweepy\n",
    "    INPUT:\n",
    "        df: original dataframe which id's we searched for\n",
    "        hashtag: hashtag we are searching for\n",
    "    OUTPUT:\n",
    "        dataframe with the columns read from the API added\n",
    "    '''\n",
    "    \n",
    "    #Initialize the id with the index values of the original dataframe\n",
    "    result = pd.DataFrame({'id': df.index.values})\n",
    "    result = result.set_index('id')\n",
    "    # create a dictionary that maps attributes of interest to ways of accessing them in a twitter object. \n",
    "    attrib_of_interest = {'tweet_location': ['place', 'name'], \\\n",
    "                          'user_location': ['user','location'], 'user_followers':['user','followers_count'], \\\n",
    "                         'retweets_No': ['retweet_count']}\n",
    "    attr_types = {'tweet_location': 'str', 'user_location': 'str', 'user_followers' : 'float64', 'retweets_No':'float64' }\n",
    "    \n",
    "    #Read the tweets\n",
    "    result =read_twitter_json_all(hashtag, result, attrib_of_interest, attr_types)\n",
    "\n",
    "    #Replace missing values\n",
    "    result['tweet_location'].replace('nan', inplace = True)\n",
    "    result['user_location'].replace('nan', inplace = True)\n",
    "    #Date column\n",
    "    result['date'] = df['date']\n",
    "\n",
    "# function that given a list of tweets and a list of attributes of interest, will fill the dataframe \n",
    "def fill_in_tweets(attributes_of_interest, raw_tweets_api, df_id_api):\n",
    "    for raw_tweet in raw_tweets_api:\n",
    "        for k,v in attrib_of_interest.items():\n",
    "            #initialisation\n",
    "            if (v):\n",
    "                attribute_value = raw_tweet[v[0]]\n",
    "            idx = 1\n",
    "            while (idx < len(v) and attribute_value):\n",
    "                attribute_value = attribute_value[v[idx]]\n",
    "                idx = idx +1 \n",
    "            if(attribute_value):\n",
    "                df_id_api.at[raw_tweet['id'], k] = attribute_value \n",
    "            else:\n",
    "                df_id_api.at[raw_tweet['id'], k] = np.nan\n",
    "            #print(attribute_value)\n",
    "    return df_id_api\n",
    "    \n",
    "# function that reads all the twitters in one json file and places them in a list  \n",
    "def read_twitter_json(filename, df_id_api, attributes_of_interest):\n",
    "    tweets_api = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for i,line in enumerate(f.readlines()):\n",
    "            tweets_api.append(json.loads(line))\n",
    "            if len(tweets_api) > 100:\n",
    "                df_id_api = fill_in_tweets(attributes_of_interest, tweets_api, df_id_api)\n",
    "                tweets_api = []   # empty the list, read other tweets. \n",
    "                \n",
    "    return df_id_api\n",
    "    \n",
    "\n",
    "# function that locates all the relevant files which include the twitters searched fo\n",
    "# and initialises the columns of interest to deafault value NAN \n",
    "def read_twitter_json_all(hashtag, df_id_api, attributes, atypes):\n",
    "    condition=False\n",
    "    df_id_api = df_id_api.reindex(columns = list(attributes.keys()) )\n",
    "    df_id_api = df_id_api.astype(atypes)\n",
    "    files_read = 0\n",
    "    for filename in os.listdir('./k/'):\n",
    "        if(hashtag=='#blacklivesmatter'):\n",
    "            condition=('tweets_app' in filename) and ('all' not in filename)\n",
    "        elif(hashtag=='#alllivesmatter'):\n",
    "            condition='tweets_app' in filename and 'all' in filename\n",
    "        if(condition):\n",
    "            #print('reading {}'.format(filename))\n",
    "            df_id_api = read_twitter_json('./k/' + filename, df_id_api,attributes )\n",
    "            files_read = files_read + 1\n",
    "    return df_id_api\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "Prior to the topics modelling and to other analysis, tweets have to be preprocessed. We use several tecniques for such preprocessing:\n",
    "* Remove slang: We replace slang words and diminutives frequently used in twitter with normal words. In order to do so, we obtain a slang dictionary by scraping https://www.noslang.com/dictionary/, which we use to replace do the replacement by calling the function replace slang. The webscraping can be found in the file *scrape_slang_dictionary.ipynb*\n",
    "* Remove handles: Remove @user and mentions\n",
    "* Reduce the length: We reduce the length of words such as tweeeeet for tweeet, so all words with more than three letters repeated get reduced to three letters and count as one word.\n",
    "* Remove stopwords: We use a dictionary of stopwords from nltk to remove all stopwords, which are not informative of the topic.\n",
    "* Remove punctuation: We remove all punctuation from the tweets\n",
    "* Remove urls: We remove all URLs from the tweets.\n",
    "* Remove RT tag: We remove the RT tag of tweets, as it should not be considered a topic.\n",
    "* Remove numbers: We remove all numbers from the tweets, as they are not informative of the topic.\n",
    "\n",
    "This preprocessing is implemented in the function *preprocess_tweet*, which given a tweet, first tokenizes it and then applies all the tecniques that are set to *True* of the above mentioned. It returns the tweet tokenized. All tweets are converted to lowercase so there is not a difference in words due to capitalization (for example, #BlackLivesMatter is the same as #blacklivesmatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_slang(token, slang):\n",
    "    '''\n",
    "    Replace slangs words with dictionary\n",
    "    INPUT:\n",
    "        token: word to replace\n",
    "        slang: slang dictionary\n",
    "    OUTPUT:\n",
    "        replaced word\n",
    "    '''\n",
    "    #if token in slang dictionary, replace it\n",
    "    try:\n",
    "        return slang[token]\n",
    "    #token not in slang dictionary\n",
    "    except:\n",
    "        return token\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet, handles=True, length=True, stop=True, punctuation=True, \n",
    "                     numbers=True, urls=True, retweet=True, slang=True, path_slang='./'):\n",
    "    '''\n",
    "    Preprocess tweet with different possibilities. \n",
    "    INPUT:\n",
    "        tweet: string with the tweet \n",
    "        handles: if true, remove handles (user mentions)\n",
    "        length: if true, reduce length of more than three characters repeated to three characters, e.g, cooool -> coool\n",
    "        stop: if true, remove stopwords\n",
    "        punctuation: if true, remove punctuation\n",
    "        numbers: if true, if true, remove numbers\n",
    "        urls: if true, remove urls\n",
    "        retweet: if true, delete RT tag.\n",
    "        slang: if true, replace slang words\n",
    "        path_slang: path to the pickled slang dictionary\n",
    "    OUTPUT:\n",
    "        list with tokenized processed tweet\n",
    "    '''\n",
    "\n",
    "    #Convert tweet to tokens, remove handles and reduce length. Convert tweet to lowercase\n",
    "    tknzr = TweetTokenizer(strip_handles=handles, reduce_len=length)\n",
    "    tweet=tknzr.tokenize(str(tweet).lower())\n",
    "    \n",
    "    #Replace stopwords\n",
    "    if stop:\n",
    "        #List of stopwords\n",
    "        stop = stopwords.words('english') \n",
    "        #add https of links and rt to stopwords\n",
    "        stop.append('https') #add https\n",
    "        stop.append('RT') #add RT\n",
    "        #Remove elemnents that are in list of stopwords\n",
    "        tweet = [token for token in tweet if token not in stop]\n",
    "        \n",
    "    #Remove punctuation\n",
    "    if punctuation:\n",
    "        #List of punctuations\n",
    "        exclude = set(string.punctuation) \n",
    "        #Remove elemnents that are in list of punctuations\n",
    "        tweet = [token for token in tweet if token not in exclude]\n",
    "    \n",
    "    #Remove numbers\n",
    "    if numbers:\n",
    "        tweet = [token for token in tweet if not (token.isdigit() \n",
    "                                         or token[0] == '-' and token[1:].isdigit())]\n",
    "    \n",
    "    #Remove URLS\n",
    "    if urls:\n",
    "        #With regexp\n",
    "        tweet=[re.sub(r'http\\S+', '', token) for token in tweet]\n",
    "    \n",
    "    #Remove retweet tag\n",
    "    if retweet:\n",
    "        tweet=[token.replace('RT:', '') for token in tweet]\n",
    "        \n",
    "    #Replace slang\n",
    "    if slang:\n",
    "        #Load slang dictionary\n",
    "        slang_dict=pickle.load(open(path_slang+'slang_dict.pkl','rb'))\n",
    "        #replace slang\n",
    "        tweet=[replace_slang(token, slang_dict) for token in tweet]\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basic'></a>\n",
    "## Basic analysis\n",
    "\n",
    "This section contains functions for plotting the number of tweets and retweets of a protest. They are two types of plots: static, which can be executed and shown inline in the notebook; and interactive, which are linked to plotly and are not shown inline.\n",
    "\n",
    "The first two function we define are general for all cases. \n",
    "* *date_restrictions*: given a dataframe and a date, removes all data from a dataframe prior to that given date. \n",
    "* *plot_events*: given a set of events (date and description), this function adds markers to a plot to indicate the most remarkable events\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_restrictions(data, date_to):\n",
    "    '''\n",
    "    Ignore data previous to date_to\n",
    "    INPUT:\n",
    "        data: dataframe with column 'date'\n",
    "        date_to: string with format YYYY-MM-DD, ignore data previous to that date\n",
    "    OUTPUT:\n",
    "        data filtered\n",
    "    '''\n",
    "    #If date_to is not empty, restrict date from it\n",
    "    if date_to:\n",
    "        data=data[data['date']> date_to]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_events(events, data_grouped, offset=300):\n",
    "    '''\n",
    "    Plot events on top of existing plot\n",
    "    \n",
    "    INPUT:\n",
    "        events: dataframe with events. One column with dates named 'date, with format 'YYYY-MM-DD'\n",
    "                and one column with the event description, named 'event'\n",
    "        data_grouped: data grouped to plot on top the events. index should be the date\n",
    "        offset: of the numbers above the circles\n",
    "        \n",
    "    '''\n",
    "    #Set column date as index\n",
    "    events=events.set_index('date')\n",
    "    \n",
    "    #Get the values in the dates or zero if there's none\n",
    "    data_events=data_grouped.get(events.index,0)\n",
    "    \n",
    "    #Plot circles on the dates and with the height of the values obtained\n",
    "    plt.plot(events.index, data_events.loc[events.index].fillna(0),'o', markeredgecolor='k', \n",
    "                                                              markerfacecolor='None',\n",
    "                                                              markersize=10)\n",
    "    \n",
    "    #Add a number and description to each event\n",
    "    for n, coor in enumerate(zip(events.index, data_events)):\n",
    "        plt.text(coor[0], coor[1]+offset, str(n), color=\"k\",clip_on=False,fontsize=14)\n",
    "        print(n, events.loc[coor[0]]['event'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of tweets\n",
    "\n",
    "These functions plot the number of tweets and retweets aggregated by day. They take a principal dataframe with its correspondent label, and optionally they can take a second dataframe and label (theoretically, of the protest against) and series of events to plot on top. If a date is given, only data starting from it onwards is plotted, that is, all previous data is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_num_tweets(data1, label1, data2=pd.DataFrame(), label2='', events=pd.DataFrame(), date_to=''):\n",
    "    '''\n",
    "    Plot of the number of tweets over time, grouped by day\n",
    "    INPUT:\n",
    "        data1: principal data to plot.\n",
    "        label1: label of data1\n",
    "        data2: secondary data (tweets against protest)\n",
    "        label2: label of data2\n",
    "        events: dataframe with the events\n",
    "        date_to: string with format YYYY-MM-DD, ignore data previous to that date\n",
    "    '''\n",
    "    #Process data 1\n",
    "    #Ignore dates before date_to\n",
    "    data1['date']=data1.copy().date.dt.to_period(\"d\") #Aggregate by day\n",
    "    #Ignore dates before date_to\n",
    "    data1=date_restrictions(data1, date_to)\n",
    "    #Group it by day and count the number\n",
    "    \n",
    "    data1_grouped=data1.groupby('date').size()\n",
    "\n",
    "    #Plot data\n",
    "    ax=data1_grouped.plot(kind='line', \n",
    "                         figsize=[15,6], \n",
    "                         label=label1,\n",
    "                         legend=True)\n",
    "    \n",
    "    #Process data2\n",
    "    if not data2.empty:\n",
    "        data1['date']=data1.copy().date.dt.to_period(\"d\") #Aggregate by day\n",
    "        #Group it by day and count the number\n",
    "        data2=date_restrictions(data2, date_to)\n",
    "        #Ignore dates before date_to\n",
    "        data2_grouped=data2.groupby('date_day').size()\n",
    "        #Plot data\n",
    "        data2_grouped.plot(ax=ax, \n",
    "                          kind='line', \n",
    "                          label=label2,\n",
    "                          legend=True,\n",
    "                          style='r',\n",
    "                          )\n",
    "    #Process events\n",
    "    if not events.empty:\n",
    "        #Ignore dates before date_to\n",
    "        events=date_restrictions(events, date_to)\n",
    "        #Plot events on top of existing plot\n",
    "        plot_events(events,data1_grouped)\n",
    "        \n",
    "    #Labels\n",
    "    ax.set_title('Number of tweets of {} protest over time'.format(label1),fontsize=14, fontweight='bold') #Title\n",
    "    ax.set_xlabel('Month',fontsize=16, fontweight='bold') #xlabel\n",
    "    ax.set_ylabel('Frequency',fontsize=16, fontweight='bold') #ylabel\n",
    "    #Change ticks parameters\n",
    "    ax.tick_params(labelsize=10) #Size\n",
    "    ax.xaxis.set_major_locator(dates.WeekdayLocator(byweekday=(1), interval=3)) #Position\n",
    "    ax.xaxis.set_major_formatter(dates.DateFormatter('%d\\n%b\\n%Y')) #Format\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functions is an interactive alternative to the previous one, for the data story. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alternative function to make it interactive\n",
    "def plot_num_tweets_interactive(data1, label1, data2=pd.DataFrame(), label2='', events=pd.DataFrame(), date_to=''):\n",
    "    '''\n",
    "    Plot of the number of tweets over time (interactive), grouped by day \n",
    "    INPUT:\n",
    "        data1: principal data to plot.\n",
    "        label1: label of data1\n",
    "        data2: secondary data (tweets against protest)\n",
    "        label2: label of data2\n",
    "        events: dataframe with the events\n",
    "        date_to: string with format YYYY-MM-DD, ignore data previous to that date\n",
    "    '''\n",
    "    #Process data 1\n",
    "    #Ignore dates before date_to\n",
    "    data1=date_restrictions(data1, date_to)\n",
    "    #Group it by day and count the number\n",
    "    data1_grouped=pd.DataFrame(data1.groupby('date').size()).reset_index()\n",
    "    #Rename columns\n",
    "    data1_grouped.columns = ['Date', 'TweetsNumber']\n",
    "    \n",
    "    #Trace for principal data\n",
    "    trace1 = go.Scatter(\n",
    "        x=data1_grouped['Date'],\n",
    "        y=data1_grouped['TweetsNumber'],\n",
    "        name=label1\n",
    "    )\n",
    "    \n",
    "    #Add to data list\n",
    "    data=[trace1]\n",
    "    \n",
    "    #Process data2\n",
    "    if not data2.empty:\n",
    "        #Group it by day and count the number\n",
    "        data2=date_restrictions(data2, date_to)\n",
    "        #Ignore dates before date_to\n",
    "        data2_grouped=pd.DataFrame(data2.groupby('date').size()).reset_index()\n",
    "        data2_grouped.columns = ['Date', 'TweetsNumber']\n",
    "    \n",
    "        #Plot\n",
    "        trace2 = go.Scatter(\n",
    "            x=data2_grouped['Date'],\n",
    "            y=data2_grouped['TweetsNumber'],\n",
    "            name=label2\n",
    "        ) \n",
    "        \n",
    "        #Add to data list\n",
    "        data.append(trace2)\n",
    "        \n",
    "    #Process events\n",
    "    if not events.empty:\n",
    "        trace3 = go.Scatter(\n",
    "            x=list(events['date']),\n",
    "            y=np.ones(len(events)) * 5,\n",
    "            mode='markers',\n",
    "            text=list(events['event']),\n",
    "            name='events',\n",
    "            marker = dict(\n",
    "                size = 10,\n",
    "                line = dict(\n",
    "                    width = 2,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        #adding the events\n",
    "        data.append(trace3)\n",
    "    \n",
    "    #Plots\n",
    "    layout = go.Layout(\n",
    "        showlegend=True,\n",
    "        title = 'Number of tweets of {} and {} over time'.format(label1,label2)\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout= layout)\n",
    "    plot_url = py.plot(fig, filename='ntweets'+label1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of retweets\n",
    "\n",
    "These functions have the same signature as the previous ones, with the only difference that in this case we plot the number of retweets grouped by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_num_retweets(data1, label1, data2=pd.DataFrame(), label2='', events=pd.DataFrame(), date_to=''):\n",
    "    \n",
    "    #Process data 1\n",
    "\n",
    "    #Limit the dates\n",
    "    data1=date_restrictions(data1, date_to)\n",
    "    #Group it by day and sum the number\n",
    "    data1_grouped=data1.groupby('date').retweets.agg('sum')\n",
    "    \n",
    "    #Plot\n",
    "    ax=data1_grouped.plot(kind='line', \n",
    "                         figsize=[15,6], \n",
    "                         label=label1,\n",
    "                         legend=True,\n",
    "                         marker='|')\n",
    "    \n",
    "    #If they pass us a second datafrmae\n",
    "    if not data2.empty:\n",
    "        #Limit the dates\n",
    "        data2=date_restrictions(data2, date_to)\n",
    "         #Group it by day and count the number\n",
    "        data2_grouped=data2.groupby('date').retweets.agg('sum')\n",
    "\n",
    "        data2_grouped.plot(ax=ax, \n",
    "                          kind='line', \n",
    "                          label=label2,\n",
    "                          legend=True,\n",
    "                          style='r',\n",
    "                          marker='|' \n",
    "                          )\n",
    "    \n",
    "    if not events.empty:\n",
    "        #Limit the dates\n",
    "        events=date_restrictions(events, date_to)\n",
    "        #Plot the events\n",
    "        plot_events(events,data1_grouped, offset=0)\n",
    "        \n",
    "    #Labels\n",
    "    ax.set_title('Number of retweets of {} protest over time'.format(label1),fontsize=14, fontweight='bold') #Title\n",
    "    ax.set_xlabel('Date',fontsize=16, fontweight='bold') #xlabel\n",
    "    ax.set_ylabel('Frequency',fontsize=16, fontweight='bold') #ylabel\n",
    "    #Change ticks parameters\n",
    "    ax.tick_params(labelsize=10) #Size\n",
    "    ax.xaxis.set_major_locator(dates.WeekdayLocator(byweekday=(1), interval=3)) #Position\n",
    "    ax.xaxis.set_major_formatter(dates.DateFormatter('%d\\n%b\\n%Y')) #Format\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alternative function to make it interactive\n",
    "def plot_num_retweets_interactive(data1, label1, data2=pd.DataFrame(), label2='', events=pd.DataFrame(), date_to=''):\n",
    "    '''\n",
    "    Plot of the number of retweets over time (interactive), grouped by day \n",
    "    INPUT:\n",
    "        data1: principal data to plot.\n",
    "        label1: label of data1\n",
    "        data2: secondary data (tweets against protest)\n",
    "        label2: label of data2\n",
    "        events: dataframe with the events\n",
    "        date_to: string with format YYYY-MM-DD, ignore data previous to that date\n",
    "    '''\n",
    "    #Process data 1\n",
    "    #Ignore dates before date_to\n",
    "    data1=date_restrictions(data1, date_to)\n",
    "    #Group it by day and count the number\n",
    "    data1_grouped=pd.DataFrame(data1.groupby('date').retweets.agg('sum')).reset_index()\n",
    "    #Rename columns to reference them better\n",
    "    data1_grouped.columns = ['Date', 'RetweetsNumber']\n",
    "    \n",
    "    # plot\n",
    "    trace1 = go.Scatter(\n",
    "        x=data1_grouped['Date'],\n",
    "        y=data1_grouped['RetweetsNumber'],\n",
    "        name=label1\n",
    "    )\n",
    "    \n",
    "    data = [trace1]\n",
    "    #Process data2\n",
    "    if not data2.empty:\n",
    "        #Group it by day and count the number\n",
    "        data2=date_restrictions(data2, date_to)\n",
    "        #Ignore dates before date_to\n",
    "        data2_grouped=pd.DataFrame(data1.groupby('date').retweets.agg('sum')).reset_index()\n",
    "        #Rename columns\n",
    "        data2_grouped.columns = ['Date', 'RetweetsNumber']\n",
    "        \n",
    "        #Plot\n",
    "        trace2 = go.Scatter(\n",
    "            x=data2_grouped['Date'],\n",
    "            y=data2_grouped['RetweetsNumber'],\n",
    "            name=label2\n",
    "        ) \n",
    "        data.append(trace2)\n",
    "    #Process events\n",
    "    if not events.empty:\n",
    "        #Ignore dates before date_to\n",
    "        events=date_restrictions(events, date_to)\n",
    "        \n",
    "        #Add markers\n",
    "        trace3 = go.Scatter(\n",
    "            x=list(events['date']),\n",
    "            y=np.ones(len(events)) * 5,\n",
    "            mode='markers',\n",
    "            text=list(events['event']),\n",
    "            name='events',\n",
    "            marker = dict(\n",
    "                size = 10,\n",
    "                line = dict(\n",
    "                    width = 2,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        data.append(trace3)\n",
    "    \n",
    "    #adding the events\n",
    "    layout = go.Layout(\n",
    "        showlegend=True,\n",
    "        title = 'Number of retweets of {} and {} over time'.format(label1,label2)\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout= layout)\n",
    "    plot_url = py.plot(fig, filename='simple-annotation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='language'></a>\n",
    "## Language analysis\n",
    "\n",
    "The functions defined in this section perform a basic analysis on the language found and translate the tweets that are not in english.\n",
    "\n",
    "For a first analysis of the language we can find in our datasets we define the function *lang_analysis* which returns the number and percentage of the tweets in each language. In addition, it also prints 3 randomly chosen tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lang_analysis(df):\n",
    "    \n",
    "    lang_count=[]\n",
    "    #Repeat analysis for all languages\n",
    "    for lang in np.unique(df.lang.dropna()):\n",
    "        tweets_lang=df.loc[df['lang'] == lang]\n",
    "        #Print the number of tweets\n",
    "        print('\\033[1m')\n",
    "        print('Number of tweets in {}: {}, {}%'.format(lang, len(tweets_lang), len(tweets_lang)/len(df)))\n",
    "        print('\\033[0m') \n",
    "        #Print 3 random tweets of each language\n",
    "        print('\\n'.join(np.random.choice(tweets_lang.text.values,3)))\n",
    "        #Print horizontal line\n",
    "        print('.'*130)\n",
    "        lang_count.append(len(tweets_lang)/len(df))\n",
    "    return lang_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the percentage of non-english tweets is very low, we still believe it may be important for obtaining the topics. Thus, we implement a function *translate* which translates a tweet into the given language. This translatation uses the API from googletrans. Given the percentage of tweets incorrectly tagged with another language when they are really in english, we find in which language is the tweet written and only translate the tweets that are not in english. It should also be mentioned that hashtags are not translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def translate(tweet, lang_id):\n",
    "    '''\n",
    "    Translate a given tweet to lang_id\n",
    "    INPUT:\n",
    "        tweet: text to translate\n",
    "        lang_id: language id\n",
    "    OUTPUT:\n",
    "        tweet translated\n",
    "    '''\n",
    "    translator = Translator()\n",
    "\n",
    "    #If tweet is in same language, do not translate\n",
    "    if (translator.detect(tweet).lang==lang_id):\n",
    "        return tweet\n",
    "\n",
    "    #Return translation\n",
    "    return translator.translate(tweet, dest=lang_id).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, due to the missing language id in the tweets from the _GetOldTweets_ API we define the function *detect_language* which given a language returns its id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_language(tweet):\n",
    "    '''\n",
    "    Detect the language of curent tweet\n",
    "    INPUT:\n",
    "        tweet: text to translate\n",
    "    OUTPUT:\n",
    "        lang_id: language id\n",
    "    '''\n",
    "    translator = Translator()\n",
    "\n",
    "    #If tweet is in same language, do not translate\n",
    "    return translator.detect(tweet).lang\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='topics'></a>\n",
    "## Topics modelling\n",
    "\n",
    "This section contains the functions used for obtaining the LDA model for topic modelling and for obtaining the topic of a tweet given the model\n",
    "\n",
    "With topic modelling, we automatically identify the topics present in the tweets. A topic is a bunch of words that tend to repeat together in a collection of tweets (corpus). We use this approach over using keywords, because we believe that as topics combine several words, they express ideas better than just one term.\n",
    "\n",
    "We use LDA (Latent Dirichlet Allocation) as our modelling tecnique to obtain lists of topics. LDA assumes that each document, or tweet, in our case is produced from a mixture of topics. Each topic is a mixture of topic terms with different weights. It has three parameters, the number of topics, the number of terms in each topic and the maximum number of iterations until convergence.  \n",
    "\n",
    "For the topic extraction, we followed the pipeline described in https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/. We use the LDA implementation from Gensim for topic modelling. We define the function *get_topics_list*, which given the dataframe containing the tweets, the number of topics to find and the preprocessing options, obtains the LDA model. In the first place, this functions calls the function defined above for the preprocessing and tokenization of the tweet. With the resulting corpus, we create the dictionary of terms and we convert it to the Document Term Matrix using the function *doc2bow*. This matrix contains the frequency of each word in each document. With this matrix, we train the LDA model calling the function *Lda*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topics_list(tweet_df, num_topics=50, handles=True, length=True, stop=True, punctuation=True, \n",
    "                     numbers=True, urls=True, retweet=True, slang=True, path_slang='./'):\n",
    "    '''\n",
    "    Return num_topics topics found in the tweets from tweet_df using LDA\n",
    "    INPUT:\n",
    "        tweet_df: dataframe containing the tweets as returned by process_tweets\n",
    "        num_topics: number of topics to find\n",
    "        handles: preprocessing option, if true, remove handles (user mentions)\n",
    "        length: preprocessing option, if true, reduce length of more than three characters repeated to three characters, e.g, cooool -> coool\n",
    "        stop: preprocessing option, if true, remove stopwords\n",
    "        punctuation: preprocessing option,  true, remove punctuation\n",
    "        numbers: preprocessing option, if true, if true, remove numbers\n",
    "        urls: preprocessing option, if true, remove urls.\n",
    "        retweet: preprocessing option, if true, delete RT tag.\n",
    "        slang: preprocessing option, if true, replace slang words\n",
    "        path_slang: preprocessing option, path to the pickled slang dictionary\n",
    "    OUTPUT:\n",
    "        trained lda model. With show_topics, each line is a topic with individual topic terms and weights. \n",
    "        dictionary used to generate the model\n",
    "    '''\n",
    "\n",
    "    #Function obtained following https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "    \n",
    "    #Preprocessing for each tweet in the dataframe\n",
    "    doc_clean = [preprocess_tweet(tweet, handles, length, stop, punctuation, \n",
    "                     numbers, urls, retweet, slang) for tweet in tweet_df.text.values]\n",
    "\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "    \n",
    "    params = {'passes': 10, 'random_state': seed} #10 passes from each word\n",
    "    # Running and Training LDA model on the document term matrix.\n",
    "    ldamodel = LdaMulticore(corpus=doc_term_matrix, num_topics=num_topics, id2word=dictionary, workers=6,\n",
    "                    passes=params['passes'], random_state=params['random_state'])\n",
    "\n",
    "    return ldamodel, dictionary, doc_term_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function that given a tweet, the LDA model and the dictionary used to generate it, obtains the underlying topics of the tweet and the probability of each of the topics. In the first place, we preprocess the tweet as before, and convert it to the Document Term Matrix using the dictionary. Finally, we obtain the list of topics for the tweet, with the format (idx, probability), where idx refers to the index of topic in the lda model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweet_topic(tweet, ldamodel, dictionary, handles=True, length=True, stop=True, punctuation=True, \n",
    "                     numbers=True, urls=True, retweet=True, slang=True, path_slang='./'):\n",
    "    \n",
    "    '''\n",
    "    Obtain the topic of an unseen tweet.\n",
    "    INPUT:\n",
    "        tweet_df: dataframe containing the tweets as returned by process_tweets\n",
    "        ldamodel: model of lda as returned by get_topics_list\n",
    "        dictionary: dictionary that created the lda model\n",
    "        handles: preprocessing option, if true, remove handles (user mentions)\n",
    "        length: preprocessing option, if true, reduce length of more than three characters repeated to three characters, e.g, cooool -> coool\n",
    "        stop: preprocessing option, if true, remove stopwords\n",
    "        punctuation: preprocessing option,  true, remove punctuation\n",
    "        numbers: preprocessing option, if true, if true, remove numbers\n",
    "        urls: preprocessing option, if true, remove urls\n",
    "        retweet: preprocessing option, if true, delete RT tag.        \n",
    "        slang: preprocessing option, if true, replace slang words\n",
    "        path_slang: preprocessing option, path to the pickled slang dictionary\n",
    "    OUTPUT:\n",
    "        topic obtained with format (idx, probability) where idx is the index of the topic in the ldamodel\n",
    "    \n",
    "    '''\n",
    "    #Preprocess tweet\n",
    "    doc_clean = preprocess_tweet(tweet, handles, length, stop, punctuation, numbers, urls, retweet, slang) \n",
    "    \n",
    "    #Convert to document term matrix\n",
    "    doc_term = dictionary.doc2bow(doc_clean)\n",
    "    \n",
    "    #Obtain the topic. Each line represents a topic and the probability\n",
    "    topic=ldamodel[doc_term]\n",
    "    \n",
    "    #Return topic\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hashtag'></a>\n",
    "## Hashtags analysis\n",
    "\n",
    "In this section we have two functions, one for analysis the coocurrence of hashtags over time and another one for generating wordclouds of hashtags. In addition to this functions, we also create an interactive graph to show the communities that can be found in the hashtag and how they are tipically linked. However, this graph is not defined in this notebook and can be found in .... \n",
    "\n",
    "_Polarization: Aparition of hashtags against and in favour together_\n",
    "\n",
    "We analyse the aparition of hashtags that are considered supportive (e.g., #BlackLivesMatter) and against (e.g., #AllLiveMatters, #PoliceLivesMatter, etc.) of the protest together, to understand whether this hashtags were not as polarized (in favour or against) as it is normally thought or whether this polarization grew over time.\n",
    "\n",
    "For this purpose, we create the function *coocurrence_hashtag* which obtains which hashtags against the protest are used with the hashtags in support. First, we obtain which of the hashtags against the protest where used with this hashtag by looking in the *hashes_inside* column for the other hashtags and adding them to a new column called *other_tag* in a new dataframe called *cooc_tags*. Then, we plot the percentage of tweets with both a hashtag for and against the protest in a bar plot, aggregated by month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cooccurence_hashtag(data, antitags, label, plot=True):\n",
    "    '''\n",
    "    Obtain tweets with co-occurence of two hashtags.\n",
    "    INPUT:\n",
    "        data: dataframe where the co-occurence has to be analysed\n",
    "        antitags: tags to look for in hashes_inside\n",
    "        label: principal hashtag we are analysing for the title\n",
    "        plot: if true, plot which percentage of tweets have both hashtags aggregated by month\n",
    "        \n",
    "    OUTPUT:\n",
    "        co-occurence dataframe with the same columns as usual and an additional column 'other_tag' which indicates which \n",
    "        tag in addition to the principal has appeared\n",
    "    '''\n",
    "    #Copy dataframe adding a column for the other tag found\n",
    "    cooc_tags=data.copy()\n",
    "    cooc_tags[\"other_tag\"] = np.nan\n",
    "    #For each tag, mark the tweets where we find the hashatg with it\n",
    "    for tag in antitags: \n",
    "        #Put tag (as lowercase) as other_tag for the tweets were tag in the other hashes, without being case sensitive.\n",
    "        cooc_tags.loc[data['hashes_inside'].str.contains(tag,case=False), 'other_tag']=tag.lower()\n",
    "    #Drop rows without any of the antitags in the text\n",
    "    cooc_tags=cooc_tags.dropna()\n",
    "    \n",
    "    #Bar plot with percentages\n",
    "    if plot: \n",
    "        #Group the dates by month\n",
    "        cooc_tags['date_month']=cooc_tags.date.dt.to_period(\"M\")\n",
    "        #Obtain the total number of tweets we have per month\n",
    "        data_by_month=data.date.dt.to_period(\"M\")\n",
    "        #Only save information where we have data from both monts\n",
    "        data_by_month=data_by_month[data_by_month.isin(cooc_tags.date_month)]\n",
    "\n",
    "        #Aggregate data by month and tag\n",
    "        tags_grouped=cooc_tags.groupby(['date_month', 'other_tag']).size().unstack()\n",
    "\n",
    "\n",
    "        #Bar plot with percentages: divide by the total data by month we have\n",
    "        ax=tags_grouped.div(data_by_month.groupby(data_by_month).size().values, axis=0).plot(kind='bar', \n",
    "                                                                                       stacked=False,  \n",
    "                                                                                       figsize=[15,4],\n",
    "                                                                                       grid=True)\n",
    "        #Legend outside the plot\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "        #Ticks and labels\n",
    "        ax.set_title('Co-occurence of {} hashtags over time'.format(label),fontsize=14, fontweight='bold') #Title\n",
    "        ax.set_xlabel('Month',fontsize=16, fontweight='bold') #xlabel\n",
    "        ax.set_ylabel('Percentage',fontsize=16, fontweight='bold') #ylabel\n",
    "        ax.tick_params(labelsize=12) #Size\n",
    "        \n",
    "    return cooc_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cooccurence_hashtag_interactive(data, antitags, label1, label2, plot=True):\n",
    "    '''\n",
    "    Obtain tweets with co-occurence of two hashtags (interactive)\n",
    "    INPUT:\n",
    "        data: dataframe where the co-occurence has to be analysed\n",
    "        antitags: tags to look for in hashes_inside\n",
    "        label1: principal hashtag we are analysing for the title\n",
    "        label2: second hashtag we are analysing for the title\n",
    "        plot: if true, plot which percentage of tweets have both hashtags aggregated by month\n",
    "        \n",
    "    OUTPUT:\n",
    "        co-occurence dataframe with the same columns as usual and an additional column 'other_tag' which indicates which \n",
    "        tag in addition to the principal has appeared\n",
    "    '''\n",
    "    #Copy dataframe adding a column for the other tag found\n",
    "    cooc_tags=data.copy()\n",
    "\n",
    "    cooc_tags[\"other_tag\"] = np.nan\n",
    "    #For each tag, mark the tweets where we find the hashatg with it\n",
    "    for tag in antitags: \n",
    "        #Put tag (as lowercase) as other_tag for the tweets were tag in the other hashes, without being case sensitive.\n",
    "        cooc_tags.loc[data['hashes_inside'].str.contains(tag,case=False), 'other_tag']=tag.lower()\n",
    "    \n",
    "    #Drop rows without any of the antitags in the text\n",
    "    cooc_tags=cooc_tags.dropna()\n",
    "\n",
    "    \n",
    "    #Bar plot with percentages\n",
    "    if plot: \n",
    "        #Group the dates by month\n",
    "        cooc_tags['date_month']=cooc_tags.date.dt.to_period(\"M\")\n",
    "        #Obtain the total number of tweets we have per month\n",
    "        data_by_month=data.date.dt.to_period(\"M\")\n",
    "        \n",
    "        #Only save information where we have data from both months\n",
    "        data_by_month=data_by_month[data_by_month.isin(cooc_tags.date_month)]\n",
    " \n",
    "\n",
    "        #Aggregate data by month and tag\n",
    "        tags_grouped=cooc_tags.groupby(['date_month', 'other_tag']).size().unstack()\n",
    "        #Obtain percentage\n",
    "        tags_grouped=pd.DataFrame(tags_grouped.div(data_by_month.groupby(data_by_month).size().values, axis=0)).reset_index()\n",
    "        plot_data=[]\n",
    "        \n",
    "        #One bar per each of the plots\n",
    "        for tag in tags_grouped.columns[1:]: \n",
    "\n",
    "            trace1 = go.Bar(\n",
    "                x=tags_grouped['date_month'].dt.to_timestamp(),\n",
    "                y=tags_grouped[tag].fillna(0),\n",
    "                name=tag\n",
    "            )\n",
    "            plot_data.append(trace1)\n",
    "\n",
    "         #Plots\n",
    "        layout = go.Layout(\n",
    "            showlegend=True,\n",
    "            yaxis = dict(title='Percentage'),\n",
    "            title = 'Co-occurence of {} and {} hashtags over time'.format(label1,label2)\n",
    "        )\n",
    "        fig = go.Figure(data=plot_data, layout= layout)\n",
    "        plot_url = py.plot(fig, filename='occurence'+label1)\n",
    "        \n",
    "\n",
    "        \n",
    "    return cooc_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sentiment'></a>\n",
    "## Sentiment analysis\n",
    "\n",
    "We perform sentiment analysis each tweet using the TextBlob. With this study we can classify tweets as:\n",
    "* Positive: if polarity is bigger than 0\n",
    "* Negative: if polarity is smaller than 0\n",
    "* Neutral: if polarity is 0\n",
    "\n",
    "In addition, we also obtain the subjectivity of the tweet which indicates whether the tweet is objective or rational (subjectivity < 0.5); subjective or emotional (subjectivity > 0.5) or neutral (subjectivity=0.5). This allows an analysis of how people react and how emotionally envolved are they.\n",
    "\n",
    "We create the function *get_tweet_sentiment*, which given a tweet returns its polarity and subjectivity. It first preprocesses the tweet, without removing the retweet tag or puntuactions, as they may be informative of the sentiment. In addition, we do not remove stopwords as they are removed by TextBlob API and convert the list of tokens to a string separated by spaces. Once preprocessed, we call TextBlob to do the analysis of the clean tweet and return the polariy and subjectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_tweet_sentiment(tweet):\n",
    "    '''\n",
    "    Get the sentiment of a tweet\n",
    "    INPUT:\n",
    "        tweet: text\n",
    "    OUTPUT:\n",
    "        polarity (-1 to 1) of the tweet\n",
    "        subjectivity(0 to 1) of the tweet\n",
    "    '''\n",
    "    #Preprocess tweet: do not remove punctuations or retweet tag. stopwords are removed by TextBlob\n",
    "    preprocessing_options_sentiment={'punctuation':False, 'retweet':False, 'stop':False}\n",
    "    clean_tweet=' '.join(preprocess_tweet(tweet, **preprocessing_options_sentiment)) #Convert list of tokens to string\n",
    "\n",
    "    # Create TextBlob object passing it the preprocessed tweet\n",
    "    analysis = TextBlob(clean_tweet)\n",
    "    \n",
    "    return analysis.sentiment.polarity, analysis.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot both the mean polarity and subjectivity of both groups of the protest aggregated by week, in order to have more consistent trends than by day. We aggregate data depending on the metric passed as a parameter, so if data is more variable we can use the median, while if it's more stable we can use the mean. The rest of the definition follows the same signature as when plotting the number of tweets and retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_sentiment_tweets(sentiment,data1, label1, data2=pd.DataFrame(), label2='', metric='mean', \n",
    "                          events=pd.DataFrame(), date_to=''):\n",
    "    '''\n",
    "    Plot of the sentiment of tweets over time, grouped by week and agreggated with the given metric\n",
    "    INPUT:\n",
    "        sentiment: name of the column with the sentiment analysis data\n",
    "        data1: principal data to plot.\n",
    "        label1: label of data1\n",
    "        data2: secondary data (tweets against protest)\n",
    "        label2: label of data2\n",
    "        metric: metric to aggregate data with\n",
    "        events: dataframe with the events\n",
    "        date_to: string with format YYYY-MM-DD, ignore data previous to that date\n",
    "    '''\n",
    "    #Ignore dates before date_to\n",
    "    data1=date_restrictions(data1.copy(), date_to)\n",
    "    \n",
    "    #Process data 1\n",
    "    data1['date_week']=data1.date.dt.to_period(\"W\") #Aggregate by week\n",
    "    \n",
    "    #Group it by day and count the number\n",
    "    data1_grouped=data1.groupby('date_week')[sentiment].agg(metric)\n",
    "\n",
    "    #Plot data\n",
    "    ax=data1_grouped.plot(kind='line', \n",
    "                         figsize=[15,6], \n",
    "                         label=label1,\n",
    "                         legend=True,\n",
    "                         marker='|')\n",
    "    \n",
    "    #Process data2\n",
    "    if not data2.empty:\n",
    "        data2=date_restrictions(data2.copy(), date_to)\n",
    "        data2['date_week']=data2.date.dt.to_period(\"W\") #Aggregate by week\n",
    "        #Group it by day and count the number\n",
    "        \n",
    "        #Ignore dates before date_to\n",
    "        data2_grouped=data2.groupby('date_week')[sentiment].agg(metric)\n",
    "        #Plot data\n",
    "        data2_grouped.plot(ax=ax, \n",
    "                          kind='line', \n",
    "                          label=label2,\n",
    "                          legend=True,\n",
    "                          style='r',\n",
    "                          marker='|'\n",
    "                          )\n",
    "    #Process events\n",
    "    if not events.empty:\n",
    "        events_c=events.copy()\n",
    "        #Ignore dates before date_to\n",
    "        events_c=date_restrictions(events_c, date_to)\n",
    "        events_c['date']=events_c.date.dt.to_period(\"W\") #Aggregate by week\n",
    "        \n",
    "        \n",
    "        #Plot events on top of existing plot\n",
    "        plot_events(events_c,data1_grouped, offset=0.001)\n",
    "        del events_c\n",
    "        \n",
    "    #Labels\n",
    "    #Title with full name of the column\n",
    "    if sentiment=='subj':\n",
    "        title_str='Subjectivity'\n",
    "    else:\n",
    "         title_str='Polarity'\n",
    "    ax.set_title('{} of tweets of {} protest over time'.format(title_str, label1),fontsize=14, fontweight='bold') #Title\n",
    "    ax.set_xlabel('Date',fontsize=16, fontweight='bold') #xlabel\n",
    "    ax.set_ylabel(title_str,fontsize=16, fontweight='bold') #ylabel\n",
    "\n",
    "    #Change ticks parameters\n",
    "    ax.tick_params(labelsize=10) #Size\n",
    "    ax.xaxis.set_major_formatter(dates.DateFormatter('%d\\n%b\\n%Y')) #Format\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_sentiment_tweets_interactive(sentiment, data1, label1, data2=pd.DataFrame(), label2='', events=pd.DataFrame(), date_to='', metric='mean'):\n",
    "    '''\n",
    "    Plot of the sentiment of tweets over time, grouped by week and agreggated with the given metric\n",
    "    INPUT:\n",
    "        sentiment: name of the column with the sentiment analysis data\n",
    "        data1: principal data to plot.\n",
    "        label1: label of data1\n",
    "        data2: secondary data (tweets against protest)\n",
    "        label2: label of data2\n",
    "\n",
    "        events: dataframe with the events\n",
    "        date_to: string with format YYYY-MM-DD, ignore data previous to that date\n",
    "        metric: metric to aggregate data with\n",
    "    '''\n",
    "    \n",
    "    if sentiment=='subj':\n",
    "        title_str='Subjectivity'\n",
    "    else:\n",
    "        title_str='Polarity'\n",
    "\n",
    "    #Process data 1\n",
    "    #Ignore dates before date_to\n",
    "    data1=date_restrictions(data1, date_to)\n",
    "    \n",
    "     #Group the dates by week\n",
    "    data1['date_week']=data1.date.dt.to_period(\"W\") #Aggregate by week\n",
    "\n",
    "    #Aggregate data by week with metric\n",
    "    data1_grouped=pd.DataFrame(data1.groupby('date_week')[sentiment].agg(metric)).reset_index()\n",
    "    #Rename columns\n",
    "    data1_grouped.columns = ['Date', 'Sentiment']\n",
    "    \n",
    "    #Trace for principal data\n",
    "    trace1 = go.Scatter(\n",
    "        x=data1_grouped['Date'].dt.to_timestamp(),\n",
    "        y=data1_grouped['Sentiment'],\n",
    "        name=label1\n",
    "    )\n",
    "   \n",
    "    #Add to data list\n",
    "    data=[trace1]\n",
    "    \n",
    "    #Process data2\n",
    "    if not data2.empty:\n",
    "        data2=date_restrictions(data2, date_to)\n",
    "        #Group it by day and count the number\n",
    "        data2['date_week']=data2.date.dt.to_period(\"W\") #Aggregate by week\n",
    "\n",
    "        #Aggregate data by week with metric\n",
    "        data2_grouped=pd.DataFrame(data2.groupby('date_week')[sentiment].agg(metric)).reset_index()\n",
    "        #Rename columns\n",
    "        data2_grouped.columns = ['Date', 'Sentiment']\n",
    "        #Plot\n",
    "        trace2 = go.Scatter(\n",
    "            x=data2_grouped['Date'].dt.to_timestamp(),\n",
    "            y=data2_grouped['Sentiment'],\n",
    "            name=label2\n",
    "        ) \n",
    "        \n",
    "        #Add to data list\n",
    "        data.append(trace2)\n",
    "        \n",
    "    #Process events\n",
    "    if not events.empty:\n",
    "        events_c=date_restrictions(events.copy(), date_to)\n",
    "        events_c['date']=events_c.date.dt.to_period(\"W\").dt.to_timestamp() #Aggregate by week\n",
    "\n",
    "        trace3 = go.Scatter(\n",
    "            x=list(events_c['date']),\n",
    "            y=np.zeros(len(events_c)),\n",
    "            mode='markers',\n",
    "            text=list(events_c['event']),\n",
    "            name='events',\n",
    "            marker = dict(\n",
    "                size = 10,\n",
    "                line = dict(\n",
    "                    width = 2,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        #adding the events\n",
    "        data.append(trace3)\n",
    "    \n",
    "    #Plots\n",
    "    layout = go.Layout(\n",
    "        showlegend=True,\n",
    "        title = '{} of tweets of {} and {} over time'.format(title_str, label1, label2),\n",
    "        yaxis = dict(title=title_str)\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout= layout)\n",
    "    plot_url = py.plot(fig, filename=title_str+label1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function *obtain_tweet_sentiment* we obtain the type of each tweet according to the sentiment passed as parameter, that is, rational, neutral or subjective if we are analysing subjectivity and postive, negative or neutral if we are analysing polarity. In addition, we also print the statistics (percentage of each type, mean and std when they are not neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obtain_type_sentiment(data, sentiment, label):\n",
    "    '''\n",
    "    Obtain the type of the sentiment of each tweet\n",
    "    INPUT:\n",
    "        sentiment: name of the column with the sentiment analysis data\n",
    "        data: dataframe\n",
    "        label: label of protest\n",
    "    OUTPUT:\n",
    "        dataframe with added column type_sent with type of sentiment: 1 (positive or rational), 0 (neutral), -1 (negative or\n",
    "        subjective)\n",
    "    '''\n",
    "    #Specify labels and limit to distinguish the types\n",
    "    if sentiment=='subj':\n",
    "        title_str='Subjectivity'\n",
    "        limit=0.5\n",
    "        neg_str='rational'\n",
    "        pos_str='subjective'\n",
    "    else:\n",
    "        limit=0\n",
    "        title_str='Polarity'\n",
    "        pos_str='positive'\n",
    "        neg_str='negative'\n",
    "    \n",
    "    data=data.copy()\n",
    "    #Initalise column\n",
    "    data['type_sent']=np.nan\n",
    "    #Positive\n",
    "    data.loc[data[sentiment]>limit, 'type_sent']=1\n",
    "    #Neutral\n",
    "    data.loc[data[sentiment]==limit, 'type_sent']=0\n",
    "    #Negative\n",
    "    data.loc[data[sentiment]<limit, 'type_sent']=-1\n",
    "    \n",
    "    #Number of total tweets \n",
    "    n_data=len(data)\n",
    "    \n",
    "    \n",
    "    #Statistics\n",
    "    print('\\033[1m'+'{} of {}'.format(title_str, label)+'\\033[0m')\n",
    "    print(\"Percentage of {} tweets: {}%\".format(pos_str, 100*len(data[data['type_sent']==1])/n_data))\n",
    "    print(\"    Mean: {}\\n    Std: {} \".format(np.mean(data.loc[data['type_sent']==1, sentiment]),\n",
    "                                               np.std(data.loc[data['type_sent']==1,sentiment])))\n",
    "    print(\"Percentage of neutral tweets: {}%\".format(100*len(data[data['type_sent']==0])/n_data))\n",
    "    print(\"Percentage of {} tweets: {}%\".format(neg_str, 100*len(data[data['type_sent']==-1])/n_data))\n",
    "    print(\"    Mean: {}\\n    Std: {} \".format(np.mean(data.loc[data['type_sent']==-1, sentiment]),\n",
    "                                               np.std(data.loc[data['type_sent']==-1, sentiment])))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*plot_type_sentiment* plots the percentage of tweets of each type of the sentiment passed as parameter over time. If events are passed, they are added to the plot. In addition, if a date is given, previous dates data is ignored and not plotted.\n",
    "\n",
    "In this case, we only plot one dataframe per plot, as each plot already has 3 lines and adding more would make it more difficult to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_type_sentiment(sentiment,data, label, events=pd.DataFrame(), date_to=''):\n",
    "    '''\n",
    "    Obtain the percentage of tweets of each type of sentiment\n",
    "    INPUT:\n",
    "        sentiment: name of the column with the sentiment analysis data\n",
    "        data: dataframe\n",
    "        label: label of protest\n",
    "        events: dataframe with the events\n",
    "        date_to: string with format YYYY-MM-DD, ignore data previous to that date\n",
    "    OUTPUT:\n",
    "        dataframe with added column type_sent with type of sentiment: 1 (positive or rational), 0 (neutral), -1 (negative or\n",
    "        subjective)\n",
    "    '''\n",
    "    #Ignore dates before date_to\n",
    "    data=date_restrictions(data.copy(), date_to)\n",
    "    \n",
    "    #Categorize sentiment in positive, neutral and negative\n",
    "    data=obtain_type_sentiment(data, sentiment, label)\n",
    "    \n",
    "    \n",
    "    #Group the dates by week\n",
    "    data['date_week']=data.date.dt.to_period(\"W\") #Aggregate by week\n",
    "\n",
    "    #Aggregate data by month and tag, and divide by the total data of month\n",
    "    sent_grouped=data.groupby(['date_week', 'type_sent']).size().unstack().div(\n",
    "                                                                            data.groupby('date_week').size().values, \n",
    "                                                                            axis=0)\n",
    "\n",
    "    if sentiment=='subj':\n",
    "        leg=['Subjective','Neutral','Rational']\n",
    "        title_str='Subjectivity'\n",
    "    else:\n",
    "        title_str='Polarity'\n",
    "        leg=['Positive','Neutral', 'Negative']\n",
    "    #Bar plot with percentages\n",
    "    ax=sent_grouped.plot(kind='line', figsize=[15,4], grid=False, marker='|')\n",
    "    #Legend outside the plot\n",
    "    plt.legend(leg,loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "\n",
    "    if not events.empty:\n",
    "\n",
    "        events_c=events.copy()\n",
    "        #Ignore dates before date_to\n",
    "        events_c=date_restrictions(events_c, date_to)\n",
    "        events_c['date']=events_c.date.dt.to_period(\"W\") #Aggregate by week\n",
    "\n",
    "        print('\\033[1m'+'Events '+'\\033[0m')\n",
    "        #Plot vertical line in each event date\n",
    "        for i, xc in enumerate(events_c['date']):\n",
    "            plt.axvline(x=xc, linestyle='--', color='k', label=i, linewidth=0.5)\n",
    "            print(i, events_c.iloc[i]['event'])\n",
    "            plt.text(xc, 0.2, i)\n",
    "\n",
    "\n",
    "        #Ticks and labels\n",
    "        ax.set_title('Distribution of {} of {} tweets over time'.format(title_str, label),fontsize=14, fontweight='bold') #Title\n",
    "        ax.set_xlabel('Month',fontsize=14, fontweight='bold') #xlabel\n",
    "        ax.set_ylabel('Percentage',fontsize=14, fontweight='bold') #ylabel\n",
    "        ax.tick_params(labelsize=12) #Size\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*plot_type_sentiment_interactive* is an alternative to the previous function, which uses and interactive plot form plotly instead of being static and inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alternative function to make it interactive\n",
    "def plot_type_sentiment_interactive(sentiment,data, label, events=pd.DataFrame(), date_to=''):\n",
    "    '''\n",
    "    Interactive plot of the percentage of tweets of each type of sentiment\n",
    "    INPUT:\n",
    "        sentiment: name of the column with the sentiment analysis data\n",
    "        data: dataframe\n",
    "        label: label of protest\n",
    "        events: dataframe with the events\n",
    "        date_to: string with format YYYY-MM-DD, ignore data previous to that date\n",
    "   \n",
    "    '''\n",
    "    #Ignore dates before date_to\n",
    "    data=date_restrictions(data.copy(), date_to)\n",
    "    \n",
    "    #Categorize sentiment in positive, neutral and negative\n",
    "    data=obtain_type_sentiment(data, sentiment, label)\n",
    "    \n",
    "    #Group the dates by week\n",
    "    data['date_week']=data.date.dt.to_period(\"W\") #Aggregate by week\n",
    "\n",
    "    #Aggregate data by month and tag, and divide by the total data of month\n",
    "    sent_grouped=data.groupby(['date_week', 'type_sent']).size().unstack().div(\n",
    "                                                                            data.groupby('date_week').size().values, \n",
    "                                                                            axis=0).reset_index()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    if sentiment=='subj':\n",
    "        sent_grouped.columns = ['Date','Subjective','Neutral','Rational']\n",
    "        title_str='Subjectivity'\n",
    "    else:\n",
    "        title_str='Polarity'\n",
    "        sent_grouped.columns = ['Date','Positive','Neutral', 'Negative']\n",
    "\n",
    "    #Process events\n",
    "    if not events.empty:\n",
    "        #Ignore dates before date_to\n",
    "        events_c=date_restrictions(events.copy(), date_to)\n",
    "        events_c['date']=events_c.date.dt.to_period(\"W\").dt.to_timestamp() #Aggregate by week\n",
    "    \n",
    "    # plot\n",
    "    #Trace of rational/postive\n",
    "    trace1 = go.Scatter(\n",
    "        x=sent_grouped['Date'].dt.to_timestamp(),\n",
    "        y=sent_grouped[sent_grouped.columns[1]], #First of three categories +date column\n",
    "        name=sent_grouped.columns[1]\n",
    "    )\n",
    "    #Trace of neutral\n",
    "    trace2 = go.Scatter(\n",
    "        x=sent_grouped['Date'].dt.to_timestamp(),\n",
    "        y=sent_grouped[sent_grouped.columns[2]],\n",
    "        name=sent_grouped.columns[2]\n",
    "    )\n",
    "    #Trace of emotional/negative\n",
    "    trace3 = go.Scatter(\n",
    "        x=sent_grouped['Date'].dt.to_timestamp(),\n",
    "        y=sent_grouped[sent_grouped.columns[3]],\n",
    "        name=sent_grouped.columns[3]\n",
    "    )\n",
    "\n",
    "    #Add to data list\n",
    "    data = [trace1, trace2, trace3]\n",
    "    #Add data of events\n",
    "    if not events.empty:\n",
    "        #Add a vertical line per event\n",
    "        shapes=[]\n",
    "        for row in events_c['date']:\n",
    "\n",
    "            shapes.append({'type': 'line',\n",
    "                           'xref': 'x',\n",
    "                           'yref': 'y',\n",
    "                           'x0': row,\n",
    "                           'y0': 0,\n",
    "                           'x1': row,\n",
    "                           'y1': 1,\n",
    "                           'opacity': 0.4})\n",
    "\n",
    "        #Trace\n",
    "        trace_events = go.Scatter(\n",
    "            x=list(events_c['date']),\n",
    "            y=np.zeros(len(events)),\n",
    "            mode='markers',\n",
    "            text=list(events_c['event']),\n",
    "            name='events',\n",
    "            marker = dict(\n",
    "                size = 10,\n",
    "                line = dict(\n",
    "                    width = 2,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "        #Add events\n",
    "        data.append(trace_events)\n",
    "    \n",
    "    #adding the events\n",
    "    layout = go.Layout(\n",
    "        barmode='group',\n",
    "        showlegend=True,\n",
    "        title = 'Distribution of {} of {} tweets over time'.format(title_str, label),\n",
    "        yaxis=dict(title='Percentage'),\n",
    "        shapes=shapes\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout= layout)\n",
    "    plot_url = py.plot(fig, filename=title_str+label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
